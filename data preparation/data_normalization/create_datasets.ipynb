{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b9792c-3e27-4e68-8198-3980e9f1c919",
      "metadata": {
        "id": "69b9792c-3e27-4e68-8198-3980e9f1c919"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4a8a15-f369-4237-b518-5101159dcfb4",
      "metadata": {
        "id": "9b4a8a15-f369-4237-b518-5101159dcfb4"
      },
      "outputs": [],
      "source": [
        "# zakaj potrebujem ponovno celotni pipeline?\n",
        "# - smo normalizirali target values? ker se lahko drugače nauči za 0.1 in 0.3\n",
        "# - smo ločili 0.1 in 0.3? Ker če ima skoraj vse iste vhodne podatke, je smiselno, da se loči?\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e72e519b-0bc3-4eba-8cb8-6a7d91bfa777",
      "metadata": {
        "id": "e72e519b-0bc3-4eba-8cb8-6a7d91bfa777"
      },
      "outputs": [],
      "source": [
        "root_folder = 'drive/MyDrive/Research/Projects/Graph_Sampling_Prediction/notebooks-export/'\n",
        "sources = {'train': root_folder + 'data/generated_graphs/aggr_data/samplings/set_1/set_1_3_6_8_9_10_with_features.csv',\n",
        "           'test_synth_medium': root_folder + 'data/generated_graphs/set_medium/all_graphs_sampling_results_with_features_v3.csv',\n",
        "           'test_synth_large': root_folder + 'data/generated_graphs/set_large/all_graphs_sampling_results_with_features_v3.csv',\n",
        "           'test_world_medium': root_folder +'data/real_graphs/set_medium/all_graphs_sampling_results_with_features.csv',\n",
        "           'test_world_large': root_folder +'data/real_graphs/set_large/all_graphs_sampling_results_with_features_v3.csv'\n",
        "           }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkG5RlY33syV",
        "outputId": "decaa2b3-bfaa-4726-b16c-8605152679d7"
      },
      "id": "DkG5RlY33syV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee611451-3231-47c4-9803-e97a0f1012f7",
      "metadata": {
        "id": "ee611451-3231-47c4-9803-e97a0f1012f7"
      },
      "outputs": [],
      "source": [
        "#df[['Real value']].hist(bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98fd58b3-15c4-4ce3-a206-097f0381867a",
      "metadata": {
        "id": "98fd58b3-15c4-4ce3-a206-097f0381867a"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(file_paths, metric):\n",
        "    train = pd.read_csv(file_paths['train'])\n",
        "    test_synth_medium = pd.read_csv(file_paths['test_synth_medium'])\n",
        "    test_synth_large = pd.read_csv(file_paths['test_synth_large'])\n",
        "    test_world_medium = pd.read_csv(file_paths['test_world_medium'])\n",
        "    test_world_large = pd.read_csv(file_paths['test_world_large'])\n",
        "\n",
        "    train['partition']='train'\n",
        "    test_synth_medium['partition']='test'\n",
        "    test_synth_large['partition']='test'\n",
        "    test_world_medium['partition']='test'\n",
        "    test_world_large['partition']='test'\n",
        "\n",
        "    train['synthetic']='synthetic'\n",
        "    test_synth_medium['synthetic']='synthetic_medium'\n",
        "    test_synth_large['synthetic']='synthetic_large'\n",
        "    test_world_medium['synthetic']='realworld_medium'\n",
        "    test_world_large['synthetic']='realworld_large'\n",
        "\n",
        "    #frames = [test_world]\n",
        "    frames = [train, test_synth_medium, test_synth_large, test_world_medium, test_world_large]\n",
        "    return pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff72c98-82d9-4626-b765-2d462a1d4744",
      "metadata": {
        "id": "3ff72c98-82d9-4626-b765-2d462a1d4744"
      },
      "outputs": [],
      "source": [
        "def max_possible_edges_not_directed(n):\n",
        "    return n*(n-1)/2\n",
        "\n",
        "# https://en.wikipedia.org/wiki/Betweenness_centrality\n",
        "def scaling_factor_node_betweenness_centrality(n):\n",
        "    return (n-1)*(n-2)/2 # undirected graphs\n",
        "\n",
        "def scaling_factor_edge_betweenness_centrality(n):\n",
        "    return (n*(n-1))/2 # undirected graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79417dd4-6772-4ee2-b96d-b2fd565eda6b",
      "metadata": {
        "id": "79417dd4-6772-4ee2-b96d-b2fd565eda6b"
      },
      "outputs": [],
      "source": [
        "def is_node_based(sampler_type):\n",
        "    if(sampler_type in set(['random degree node', 'random node', 'random node edge'])):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6d0ea2-5a23-4175-8110-6bdf339d94d6",
      "metadata": {
        "id": "4b6d0ea2-5a23-4175-8110-6bdf339d94d6"
      },
      "outputs": [],
      "source": [
        "def is_edge_based(sampler_type):\n",
        "    if(sampler_type in set(['random edge', 'random node edge', 'induced random edge'])):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6483b091-7223-427e-9184-c5d41f5ae09e",
      "metadata": {
        "id": "6483b091-7223-427e-9184-c5d41f5ae09e"
      },
      "outputs": [],
      "source": [
        "def is_traversal_based(sampler_type):\n",
        "    if(sampler_type in set(['random jump', 'snowball', 'forest fire', 'metropolis hastings random walk', 'expansion', 'frontier', 'rank degree'])):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0ef372-0a4f-4f94-adf0-cdd02ec8c7dd",
      "metadata": {
        "id": "3d0ef372-0a4f-4f94-adf0-cdd02ec8c7dd"
      },
      "outputs": [],
      "source": [
        "def add_sampler_type(df):\n",
        "    df['sampler_type']=df.apply(lambda row: get_sampler_type(row['sampling algorithm']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f438830d-8cc6-4a39-8bd3-de41b9bb6ec9",
      "metadata": {
        "id": "f438830d-8cc6-4a39-8bd3-de41b9bb6ec9"
      },
      "outputs": [],
      "source": [
        "def get_graph_param(graph_id):\n",
        "    if '_Syn_' in graph_id:\n",
        "        values = graph_id.split('range_size')[1].split('_param:')\n",
        "        values = [values[0]] + values[1].split('_')\n",
        "        return values[1]\n",
        "    return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a6eca65-30bb-4961-a340-5734d0ebf9c7",
      "metadata": {
        "id": "8a6eca65-30bb-4961-a340-5734d0ebf9c7"
      },
      "outputs": [],
      "source": [
        "def generate_features(df):\n",
        "    #df['graph param'] = df.apply(lambda row: get_graph_param(row['graph id']), axis=1)\n",
        "\n",
        "    mapping = {1:'snowball', 2:'random node', 3:'metropolis hastings random walk', 4: 'random degree node', 5: 'random jump', 6: 'random edge', 7: 'random node edge', 8: 'forest fire', 9:'expansion', 10: 'frontier', 11:'induced random edge', 12:'rank degree'}\n",
        "\n",
        "    #df['sampling_algorithm'] = df['sample_algs'].map(mapping)\n",
        "\n",
        "    df['node_count/edge_count']=np.exp(-np.log((df['node_nums']/df['edge_nums'])+1))\n",
        "    df['edge_count/node_count']=np.exp(-np.log((df['edge_nums']/df['node_nums'])+1))\n",
        "\n",
        "    df['clust_coeff_max']=np.exp(-np.log(df['max_clust_coeff']+1))\n",
        "    df['clust_coeff_min']=df['min_clust_coeff']/df['max_clust_coeff']\n",
        "    df['clust_coeff_avg']=df['mean_clust_coeff']/df['max_clust_coeff']\n",
        "    df['clust_coeff_var']=np.exp(-np.log(df['var_clust_coeff']+1))\n",
        "    df['clust_coeff_median']=df['median_clust_coeff']/df['max_clust_coeff']\n",
        "\n",
        "    df['scaling_factor_node_betweenness_centrality']=df.apply(lambda row: max_possible_edges_not_directed(row['node_nums']), axis=1)\n",
        "    #df['scaling_factor_edge_betweenness_centrality']=df.apply(lambda row: max_possible_edges_not_directed(row['node_nums']), axis=1)\n",
        "\n",
        "    df['degree_min']=df['min_degree']/df['max_degree']\n",
        "    df['degree_avg']=df['mean_degree']/df['max_degree']\n",
        "    df['degree_max']=np.exp(-np.log(df['max_degree']+1))\n",
        "    # see: https://math.stackexchange.com/questions/2833062/a-measure-similar-to-variance-thats-always-between-0-and-1\n",
        "    # we add log to alleviate how quickly the value approximates zero\n",
        "    df['degree_var']=np.exp(-np.log(df['var_degree']+1))\n",
        "    df['degree_median']=df['median_degree']/df['max_degree']\n",
        "\n",
        "    #'graph_density', 'min_clust_coeff', ''\n",
        "    # 'mean_clust_coeff', 'var_clust_coeff', 'median_clust_coeff'\n",
        "    df['node_betweenness_centrality_max']=np.exp(-(df['max_node_betweenness_centrality']/df['scaling_factor_node_betweenness_centrality']))\n",
        "    df['node_betweenness_centrality_avg']=df['mean_node_betweenness_centrality']/df['scaling_factor_node_betweenness_centrality']\n",
        "    df['node_betweenness_centrality_var']=np.exp(-np.log(df['var_node_betweenness_centrality']+1))\n",
        "    df['node_betweenness_centrality_median']=df['median_node_betweenness_centrality']/df['scaling_factor_node_betweenness_centrality']\n",
        "    df['node_betweenness_centrality_min']=df['min_node_betweenness_centrality']/df['scaling_factor_node_betweenness_centrality']\n",
        "\n",
        "    '''\n",
        "    df['edge_betweenness_centrality_max']=np.exp(-(df['max_edge_betweenness_centrality']/df['scaling_factor_edge_betweenness_centrality']))\n",
        "    df['edge_betweenness_centrality_avg']=df['mean_edge_betweenness_centrality']/df['scaling_factor_edge_betweenness_centrality']\n",
        "    df['edge_betweenness_centrality_var']=np.exp(-np.log(df['var_edge_betweenness_centrality']+1))\n",
        "    df['edge_betweenness_centrality_median']=df['median_edge_betweenness_centrality']/df['scaling_factor_edge_betweenness_centrality']\n",
        "\n",
        "    df['eccentricity_centrality_min']=df['min_eccentricity_centrality']/df['max_eccentricity_centrality']\n",
        "    df['eccentricity_centrality_avg']=df['mean_eccentricity_centrality']/df['max_eccentricity_centrality']\n",
        "    df['eccentricity_centrality_var']=np.exp(-np.log(df['var_eccentricity_centrality']+1))\n",
        "    df['eccentricity_centrality_median']=df['median_eccentricity_centrality']/df['max_eccentricity_centrality']\n",
        "    '''\n",
        "\n",
        "    df['eigenvector_centrality_min']=df['min_eigenvector_centrality']/df['max_eigenvector_centrality']\n",
        "    df['eigenvector_centrality_avg']=df['mean_eigenvector_centrality']/df['max_eigenvector_centrality']\n",
        "    df['eigenvector_centrality_median']=df['median_eigenvector_centrality']/df['max_eigenvector_centrality']\n",
        "    df['eigenvector_centrality_var']=np.exp(-np.log(df['var_eigenvector_centrality']+1))\n",
        "    df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n",
        "\n",
        "    df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
        "    df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
        "    df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
        "    df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
        "    df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n",
        "\n",
        "    df['min_connected_components_size'] = df['min_connected_components_size']/df['max_connected_components_size']\n",
        "    df['mean_connected_components_size'] = df['mean_connected_components_size']/df['max_connected_components_size']\n",
        "    df['median_connected_components_size'] = df['median_connected_components_size']/df['max_connected_components_size']\n",
        "    df['var_connected_components_size'] = np.exp(-np.log(df['var_connected_components_size']+1))\n",
        "    df['num_connected_components'] = np.exp(-np.log(df['num_connected_components']+1))\n",
        "    df['max_connected_components_size'] = np.exp(-np.log(df['max_connected_components_size']+1))\n",
        "\n",
        "    df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n",
        "    df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n",
        "    df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n",
        "    df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n",
        "    df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n",
        "\n",
        "    df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n",
        "    df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n",
        "    df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n",
        "    df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n",
        "    '''\n",
        "    df['shortest_path_length_lcc_min'] = df['min_shortest_path_length_LCC']/df['max_shortest_path_length_LCC']\n",
        "    df['shortest_path_length_lcc_mean'] = df['mean_shortest_path_length_LCC']/df['max_shortest_path_length_LCC']\n",
        "    df['shortest_path_length_lcc_var'] = np.exp(-np.log(df['var_shortest_path_length_LCC']+1))\n",
        "    df['shortest_path_length_lcc_max']=np.exp(-np.log(df['max_shortest_path_length_LCC']+1))\n",
        "    '''\n",
        "    df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n",
        "    df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n",
        "    df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n",
        "\n",
        "    # calc time features\n",
        "    df['clust_coeff_calc_time'] = np.exp(-np.log(df['clust_coeff_calc_time']+1))\n",
        "    df['connected_components_calc_time'] = np.exp(-np.log(df['connected_components_calc_time']+1))\n",
        "    df['degree_assortativity_calc_time'] = np.exp(-np.log(df['degree_assortativity_calc_time']+1))\n",
        "    df['eigenvector_centrality_calc_time'] = np.exp(-np.log(df['eigenvector_centrality_calc_time']+1))\n",
        "    df['max_spanning_tree_calc_time'] = np.exp(-np.log(df['max_spanning_tree_calc_time']+1))\n",
        "    df['pagerank_centrality_calc_time'] = np.exp(-np.log(df['pagerank_centrality_calc_time']+1))\n",
        "\n",
        "    # size features\n",
        "    # model 1\n",
        "    #df['node_nums'] = df['node_nums']/1000000\n",
        "    #df['edge_nums'] = df['edge_nums']/100000000\n",
        "\n",
        "    # model 2\n",
        "    df['node_nums'] = np.exp(-np.log(df['node_nums']+1))\n",
        "    df['edge_nums'] = np.exp(-np.log(df['edge_nums']+1))\n",
        "\n",
        "    #print('df shape before join ', df.shape)\n",
        "    #print('df index before join ', df.columns)\n",
        "    #one_hot = pd.get_dummies(df['sampling_algorithm']).replace({False: 0, True: 1})\n",
        "    #print('one_hot indx ', one_hot.index)\n",
        "    #print('one_hot ', one_hot)\n",
        "    df['sampling algorithm'] = df['sampling_algorithm']\n",
        "    df = pd.get_dummies(df, columns=['sampling_algorithm'], prefix='', prefix_sep='').replace({False: 0, True: 1})\n",
        "    return df\n",
        "    #return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfa3126-180a-4d86-a020-6341999b78ef",
      "metadata": {
        "id": "1dfa3126-180a-4d86-a020-6341999b78ef"
      },
      "outputs": [],
      "source": [
        "def data_quality_check(df):\n",
        "    nalist = df.columns[df.isna().any()].tolist()\n",
        "    print('nalist ', nalist)\n",
        "    if len(nalist)!=0:\n",
        "        print(df[df['graph_ID'].isnull()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44aafc90-091b-41f0-83de-ce2215138e27",
      "metadata": {
        "id": "44aafc90-091b-41f0-83de-ce2215138e27"
      },
      "outputs": [],
      "source": [
        "def now_vs_after(df):\n",
        "    print('Now: {}, after: {}'.format(len(df.index), len(df.drop_duplicates().index)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fef567b-2ee5-4a45-9a88-9d71dfbca865",
      "metadata": {
        "id": "2fef567b-2ee5-4a45-9a88-9d71dfbca865",
        "outputId": "5e65d73c-9701-434d-d1d7-bb57c88a85a7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#df = generate_dataset(sources, target)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m generate_features(\u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset \u001b[39m\u001b[38;5;124m'\u001b[39m, dataset)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mcolumns)\n",
            "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[0;34m(file_paths, metric)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_dataset\u001b[39m(file_paths, metric):\n\u001b[0;32m----> 2\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(file_paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#test_synth = pd.read_csv(file_paths['test_synth'])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#test_world = pd.read_csv(file_paths['test_world'])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "target='D3'\n",
        "#df = generate_dataset(sources, target)\n",
        "dataset = generate_features(generate_dataset(sources, target)).drop_duplicates().fillna(0)\n",
        "print('dataset ', dataset)\n",
        "print(dataset.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dfbc391-8493-441e-9425-581ba2278d1b",
      "metadata": {
        "id": "2dfbc391-8493-441e-9425-581ba2278d1b"
      },
      "outputs": [],
      "source": [
        "# quality check - ensure all values are between zero and one\n",
        "for feature in ['node_count/edge_count', 'edge_count/node_count', 'clust_coeff_max', 'clust_coeff_min', 'clust_coeff_avg',\n",
        "            'clust_coeff_var', 'clust_coeff_median', 'degree_min',\n",
        "            'degree_avg', 'degree_var', 'degree_median', 'node_betweenness_centrality_max', 'node_betweenness_centrality_avg',\n",
        "            'node_betweenness_centrality_var', 'node_betweenness_centrality_median',\n",
        "            'edge_betweenness_centrality_max', 'edge_betweenness_centrality_avg',\n",
        "            'edge_betweenness_centrality_var', 'edge_betweenness_centrality_median', 'eccentricity_centrality_min',\n",
        "            'eccentricity_centrality_avg', 'eccentricity_centrality_var', 'eccentricity_centrality_median',\n",
        "            'eigenvector_centrality_min', 'eigenvector_centrality_var', 'eigenvector_centrality_avg',\n",
        "            'pagerank_centrality_var', 'degrees_spanning_tree_min', 'degrees_spanning_tree_avg', 'degrees_spanning_tree_var',\n",
        "            'min_pagerank_centrality', 'max_pagerank_centrality', 'mean_pagerank_centrality', 'median_pagerank_centrality',\n",
        "            'graph_density']:\n",
        "    if dataset[feature].max()>1:\n",
        "        print('Feature: {}, value: {}'.format(feature, dataset[feature].max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8112e73-0861-4eb9-977f-94e025572483",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8112e73-0861-4eb9-977f-94e025572483",
        "outputId": "b58d154e-2de6-4c54-9340-c2872a6a98aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: D3\n",
            "nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n",
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 98 columns]\n",
            "Now: 14143, after: 14143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampling algorithm'] = df['sampling_algorithm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now: 14143, after: 14143\n",
            "Target: C2D2\n",
            "nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n",
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 98 columns]\n",
            "Now: 14143, after: 14143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampling algorithm'] = df['sampling_algorithm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now: 14143, after: 14143\n",
            "Target: HPD2\n",
            "nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n",
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 98 columns]\n",
            "Now: 14143, after: 14143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampling algorithm'] = df['sampling_algorithm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now: 14143, after: 14143\n",
            "Target: HPD2_LCC\n",
            "nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n",
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 98 columns]\n",
            "Now: 14143, after: 14143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampling algorithm'] = df['sampling_algorithm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now: 14143, after: 14143\n",
            "Target: run_time\n",
            "nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n",
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 98 columns]\n",
            "Now: 14143, after: 14143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n",
            "<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n",
            "<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n",
            "<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n",
            "<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n",
            "<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df['sampling algorithm'] = df['sampling_algorithm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now: 14143, after: 14143\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "model_num = '2'\n",
        "\n",
        "for target in ['D3', 'C2D2', 'HPD2', 'HPD2_LCC', 'run_time']:\n",
        "    dataset = generate_dataset(sources, target)\n",
        "\n",
        "    # data quality check\n",
        "    print('Target: {}'.format(target))\n",
        "    data_quality_check(dataset)\n",
        "    now_vs_after(dataset)\n",
        "\n",
        "    dfx = generate_features(dataset).drop_duplicates().fillna(0)\n",
        "    now_vs_after(dfx)\n",
        "    all_algorithms = ['forest fire', 'random degree node', 'random edge', 'random jump', 'random node', 'random node edge', 'snowball', 'frontier', 'rank degree', 'induced random edge', 'metropolis hastings random walk', 'expansion']\n",
        "    for alg in all_algorithms:\n",
        "        if not alg in dfx.columns: # alg not in df\n",
        "            dfx[alg] = 0\n",
        "    dfx = dfx.rename(columns={\"KS Degree Distr\": \"D3\", \"KS Clustering Coefficient Distr\": \"C2D2\", 'KS hop plots Distr': 'HPD2', 'KS hop plots LCC Distr': 'HPD2_LCC'})\n",
        "    dfx.to_csv(root_folder + 'data/model_' + model_num + '/{}_v4.csv'.format(target), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3e7027-93b7-488f-91a1-7b571bf26958",
      "metadata": {
        "id": "8f3e7027-93b7-488f-91a1-7b571bf26958",
        "outputId": "8938367c-8c2d-4929-aeb3-f2bb240bf948"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clust_coeff_calc_time</th>\n",
              "      <th>connected_components_calc_time</th>\n",
              "      <th>pagerank_centrality_calc_time</th>\n",
              "      <th>max_spanning_tree_calc_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.155451</td>\n",
              "      <td>0.440589</td>\n",
              "      <td>0.454012</td>\n",
              "      <td>1.744806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.067737</td>\n",
              "      <td>0.920141</td>\n",
              "      <td>0.321629</td>\n",
              "      <td>9.598539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.018222</td>\n",
              "      <td>0.765105</td>\n",
              "      <td>0.528590</td>\n",
              "      <td>6.645228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.130755</td>\n",
              "      <td>0.269564</td>\n",
              "      <td>0.256353</td>\n",
              "      <td>4.912762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.205690</td>\n",
              "      <td>0.193303</td>\n",
              "      <td>0.760978</td>\n",
              "      <td>3.227988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>9.447464</td>\n",
              "      <td>0.664761</td>\n",
              "      <td>0.824831</td>\n",
              "      <td>11.493466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>7.822001</td>\n",
              "      <td>3.362185</td>\n",
              "      <td>0.783072</td>\n",
              "      <td>17.071564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>4.993967</td>\n",
              "      <td>0.087790</td>\n",
              "      <td>0.752546</td>\n",
              "      <td>6.690350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>5.315836</td>\n",
              "      <td>0.321391</td>\n",
              "      <td>0.650829</td>\n",
              "      <td>11.097322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>14.282801</td>\n",
              "      <td>0.265360</td>\n",
              "      <td>0.530702</td>\n",
              "      <td>40.759410</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>487 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     clust_coeff_calc_time  connected_components_calc_time  \\\n",
              "0                 0.155451                        0.440589   \n",
              "0                 2.067737                        0.920141   \n",
              "1                 2.018222                        0.765105   \n",
              "1                 1.130755                        0.269564   \n",
              "2                 0.205690                        0.193303   \n",
              "..                     ...                             ...   \n",
              "475               9.447464                        0.664761   \n",
              "476               7.822001                        3.362185   \n",
              "477               4.993967                        0.087790   \n",
              "478               5.315836                        0.321391   \n",
              "479              14.282801                        0.265360   \n",
              "\n",
              "     pagerank_centrality_calc_time  max_spanning_tree_calc_time  \n",
              "0                         0.454012                     1.744806  \n",
              "0                         0.321629                     9.598539  \n",
              "1                         0.528590                     6.645228  \n",
              "1                         0.256353                     4.912762  \n",
              "2                         0.760978                     3.227988  \n",
              "..                             ...                          ...  \n",
              "475                       0.824831                    11.493466  \n",
              "476                       0.783072                    17.071564  \n",
              "477                       0.752546                     6.690350  \n",
              "478                       0.650829                    11.097322  \n",
              "479                       0.530702                    40.759410  \n",
              "\n",
              "[487 rows x 4 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfx[['clust_coeff_calc_time',\n",
        "       'connected_components_calc_time', 'pagerank_centrality_calc_time',\n",
        "       'max_spanning_tree_calc_time']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2698dbd9-042a-417b-a379-990052391f18",
      "metadata": {
        "id": "2698dbd9-042a-417b-a379-990052391f18"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}